# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hcf9gx_ShYtvDAJx5EvzIhed43SXREKp
"""

# Step 1.1: Load the MNIST dataset using keras.datasets.mnist
from tensorflow import keras
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
print("Loaded MNIST.")

# Step 1.2: Print the shape of x_train, y_train, x_test, and y_test
print("x_train shape:", x_train.shape)
print("y_train shape:", y_train.shape)
print("x_test  shape:", x_test.shape)
print("y_test  shape:", y_test.shape)

# Step 1.3: Display the first image from x_train using matplotlib
import matplotlib.pyplot as plt
plt.imshow(x_train[0], cmap='gray')
plt.axis('off')
plt.show()

# Step 1.4: Print the label of the first image
print("Label of first image:", y_train[0])

# Step 2.5: Normalize the pixel values by dividing by 255
x_train_norm = x_train.astype('float32') / 255.0
x_test_norm  = x_test.astype('float32')  / 255.0
print("Normalization done.")

# Step 2.6: Print the first image’s pixel values before and after normalization
print("Before (sample 5x5 of top-left):")
print(x_train[0][:5,:5])
print("\nAfter normalization (same 5x5):")
print(x_train_norm[0][:5,:5])

# Step 3.7-3.11: Build the Sequential model (Flatten -> Dense(128, relu) -> Dense(64, relu) -> Dense(10, softmax))
from tensorflow.keras import models, layers
model = models.Sequential()
model.add(layers.Flatten(input_shape=(28,28)))         # Step 3.8
model.add(layers.Dense(128, activation='relu'))        # Step 3.9
model.add(layers.Dense(64, activation='relu'))         # Step 3.10
model.add(layers.Dense(10, activation='softmax'))      # Step 3.11
print("Model built.")

# Step 3.12: Print the model summary
model.summary()

# Step 4.13: Compile the model with optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
print("Model compiled.")

# Step 4.14: One-line explanation (printed)
print("optimizer='adam' -> algorithm to update weights; loss='sparse_categorical_crossentropy' -> measures error for integer labels; metrics=['accuracy'] -> tracks accuracy during training.")

# Step 5.15: Train the model for 5 epochs
history_5 = model.fit(x_train_norm, y_train, epochs=5, batch_size=32, validation_split=0.1)

# Step 5.16: Note down the training accuracy after the last epoch (print it)
train_acc_last = history_5.history['accuracy'][-1]
print("Training accuracy after last epoch (5 epochs):", train_acc_last)

# Step 5.17: Train for 10 epochs (continue training on same model) and observe change in accuracy
history_10 = model.fit(x_train_norm, y_train, epochs=10, batch_size=32, validation_split=0.1)
print("Training accuracy after last epoch (10 epochs):", history_10.history['accuracy'][-1])

# Step 6.18-6.19: Evaluate the model on x_test and print test accuracy
test_loss, test_acc = model.evaluate(x_test_norm, y_test, verbose=0)
print("Test loss:", test_loss)
print("Test accuracy:", test_acc)

# Step 6.20: Compare training accuracy vs test accuracy (print both)
print("Last training accuracy:", history_10.history['accuracy'][-1])
print("Test accuracy:", test_acc)
print("Which is higher? ->", "Training" if history_10.history['accuracy'][-1] > test_acc else "Test/Equal")

# Step 7.21: Use model.predict() on x_test to get predictions
pred_probs = model.predict(x_test_norm)   # shape: (10000, 10)
print("Predictions shape:", pred_probs.shape)

# Step 7.22: Print the prediction result (probabilities) for the first test image
print("Predicted probabilities for first test image:\n", pred_probs[0])

# Step 7.23: Use tf.argmax() to find which digit was predicted for the first test image
import numpy as np
pred_label_first = np.argmax(pred_probs[0])
print("Predicted digit (first test image):", pred_label_first)

# Step 7.24: Print both predicted and actual label for the first image
print("Predicted:", pred_label_first, "| Actual:", y_test[0])

# Step 7.25: Display the first test image with its predicted digit as the title
plt.imshow(x_test[0], cmap='gray')
plt.title(f"Predicted: {pred_label_first} | Actual: {y_test[0]}")
plt.axis('off')
plt.show()

# Step 7.26: Add one more hidden layer with 32 neurons and retrain (build fresh model)
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten, Dense

model2 = Sequential([
    Flatten(input_shape=(28,28)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),   # added layer
    Dense(10, activation='softmax')
])
model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history_model2 = model2.fit(x_train_norm, y_train, epochs=5, batch_size=32, validation_split=0.1)
print("Finished training model with extra 32-neuron layer.")

# Step 7.27: Change optimizer to 'sgd' and note accuracy difference (build fresh model with same architecture as previous)
model_sgd = Sequential([
    Flatten(input_shape=(28,28)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
model_sgd.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history_sgd = model_sgd.fit(x_train_norm, y_train, epochs=5, batch_size=32, validation_split=0.1)
print("SGD training finished. Last training accuracy:", history_sgd.history['accuracy'][-1])

# Step 7.28: Train the model with only 1 epoch — what happens to accuracy?
model_one_epoch = Sequential([
    Flatten(input_shape=(28,28)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
model_one_epoch.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history_one = model_one_epoch.fit(x_train_norm, y_train, epochs=1, batch_size=32, validation_split=0.1)
print("Training accuracy after 1 epoch:", history_one.history['accuracy'][-1])

# Step 7.29: Change activation from 'relu' to 'tanh' and retrain (example: one model)
model_tanh = Sequential([
    Flatten(input_shape=(28,28)),
    Dense(128, activation='tanh'),
    Dense(64, activation='tanh'),
    Dense(10, activation='softmax')
])
model_tanh.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history_tanh = model_tanh.fit(x_train_norm, y_train, epochs=5, batch_size=32, validation_split=0.1)
print("Last training acc with tanh:", history_tanh.history['accuracy'][-1])

# Step 7.30: Plot training accuracy vs. epoch using matplotlib (example using history_5 or any history object)
import matplotlib.pyplot as plt
acc = history_5.history['accuracy']
val_acc = history_5.history.get('val_accuracy')
epochs = range(1, len(acc)+1)
plt.plot(epochs, acc, label='train acc')
if val_acc:
    plt.plot(epochs, val_acc, label='val acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training accuracy vs Epoch')
plt.legend()
plt.show()

